{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b9301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enrique/.pyenv/versions/3.10.6/envs/SolarSoundBytes/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# FNSPID Dataset Download and Setup\n",
    "# Dataset: 15.7M financial news articles with sentiment labels (1999-2023)\n",
    "# Source: https://huggingface.co/datasets/Zihan1004/FNSPID\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from huggingface_hub import hf_hub_download\n",
    "import zipfile\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62228de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading FNSPID Financial News Dataset...\n",
      "This is a large dataset (~30GB), ensure you have sufficient storage\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Download FNSPID Financial News Dataset\n",
    "print(\"Downloading FNSPID Financial News Dataset...\")\n",
    "print(\"This is a large dataset (~30GB), ensure you have sufficient storage\")\n",
    "\n",
    "# Download methods (choose one):\n",
    "\n",
    "# Method 1: Direct download using wget (Linux/Mac)\n",
    "\"\"\"\n",
    "# Run these commands in terminal:\n",
    "wget https://huggingface.co/datasets/Zihan1004/FNSPID/resolve/main/Stock_news/nasdaq_exteral_data.csv\n",
    "wget https://huggingface.co/datasets/Zihan1004/FNSPID/resolve/main/Stock_news/sentiment_scored_news.zip\n",
    "\"\"\"\n",
    "\n",
    "# Method 2: Python download\n",
    "def download_fnspid_news():\n",
    "    \"\"\"Download FNSPID news data files\"\"\"\n",
    "\n",
    "    base_url = \"https://huggingface.co/datasets/Zihan1004/FNSPID/resolve/main/Stock_news/\"\n",
    "    files_to_download = [\n",
    "        \"nasdaq_exteral_data.csv\",\n",
    "        \"sentiment_scored_news.zip\"\n",
    "    ]\n",
    "\n",
    "    for filename in files_to_download:\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        url = base_url + filename\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"✅ Downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to download {filename}\")\n",
    "\n",
    "# Step 2: Load and explore the dataset\n",
    "def load_fnspid_data():\n",
    "    \"\"\"Load and explore FNSPID news data\"\"\"\n",
    "\n",
    "    # Load main news data\n",
    "    print(\"Loading FNSPID news data...\")\n",
    "    df_news = pd.read_csv(\"nasdaq_exteral_data.csv\")\n",
    "\n",
    "    print(f\"Dataset shape: {df_news.shape}\")\n",
    "    print(f\"Columns: {df_news.columns.tolist()}\")\n",
    "    print(f\"Date range: {df_news['date'].min()} to {df_news['date'].max()}\")\n",
    "\n",
    "    # Show sample data\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df_news.head())\n",
    "\n",
    "    # Check sentiment distribution\n",
    "    if 'sentiment' in df_news.columns:\n",
    "        print(f\"\\nSentiment distribution:\")\n",
    "        print(df_news['sentiment'].value_counts())\n",
    "\n",
    "    return df_news\n",
    "\n",
    "# Step 3: Filter for recent data (2022-2024)\n",
    "def filter_recent_data(df, start_year=2022, end_year=2024):\n",
    "    \"\"\"Filter dataset for specific years\"\"\"\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "\n",
    "    # Filter for your target years\n",
    "    df_filtered = df[(df['year'] >= start_year) & (df['year'] <= end_year)]\n",
    "\n",
    "    print(f\"Filtered data ({start_year}-{end_year}):\")\n",
    "    print(f\"Shape: {df_filtered.shape}\")\n",
    "    print(f\"Articles per year:\")\n",
    "    print(df_filtered['year'].value_counts().sort_index())\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# Step 4: Prepare data for DistilBERT training\n",
    "def prepare_training_data(df):\n",
    "    \"\"\"Prepare FNSPID data for DistilBERT fine-tuning\"\"\"\n",
    "\n",
    "    # Create text column combining title and content\n",
    "    if 'title' in df.columns and 'text' in df.columns:\n",
    "        df['combined_text'] = df['title'].fillna('') + \". \" + df['text'].fillna('')\n",
    "    elif 'news_text' in df.columns:\n",
    "        df['combined_text'] = df['news_text'].fillna('')\n",
    "    else:\n",
    "        # Adapt based on actual column names\n",
    "        text_cols = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower()]\n",
    "        if text_cols:\n",
    "            df['combined_text'] = df[text_cols[0]].fillna('')\n",
    "\n",
    "    # Clean sentiment labels (adapt based on actual format)\n",
    "    # FNSPID uses various sentiment scoring methods - normalize to positive/neutral/negative\n",
    "    if 'sentiment_score' in df.columns:\n",
    "        # Convert sentiment scores to labels\n",
    "        df['sentiment_label'] = df['sentiment_score'].apply(lambda x:\n",
    "            'positive' if x > 0.1 else ('negative' if x < -0.1 else 'neutral'))\n",
    "    elif 'sentiment' in df.columns:\n",
    "        df['sentiment_label'] = df['sentiment']\n",
    "\n",
    "    # Filter out articles that are too short or too long\n",
    "    df['text_length'] = df['combined_text'].str.len()\n",
    "    df_clean = df[\n",
    "        (df['text_length'] >= 100) &  # Minimum 100 characters\n",
    "        (df['text_length'] <= 2000) & # Maximum 2000 characters\n",
    "        (df['combined_text'].notna()) &\n",
    "        (df['sentiment_label'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "    print(f\"Text length stats:\")\n",
    "    print(df_clean['text_length'].describe())\n",
    "    print(f\"\\nSentiment distribution:\")\n",
    "    print(df_clean['sentiment_label'].value_counts())\n",
    "\n",
    "    return df_clean[['combined_text', 'sentiment_label', 'date']]\n",
    "\n",
    "# Step 5: Sample balanced training data\n",
    "def create_balanced_sample(df, samples_per_class=5000):\n",
    "    \"\"\"Create a balanced sample for training\"\"\"\n",
    "\n",
    "    balanced_dfs = []\n",
    "\n",
    "    for sentiment in df['sentiment_label'].unique():\n",
    "        sentiment_df = df[df['sentiment_label'] == sentiment]\n",
    "        if len(sentiment_df) >= samples_per_class:\n",
    "            sampled = sentiment_df.sample(n=samples_per_class, random_state=42)\n",
    "        else:\n",
    "            sampled = sentiment_df  # Use all available if less than target\n",
    "        balanced_dfs.append(sampled)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "    print(f\"Balanced training set shape: {balanced_df.shape}\")\n",
    "    print(f\"Sentiment distribution:\")\n",
    "    print(balanced_df['sentiment_label'].value_counts())\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*50)\n",
    "    print(\"FNSPID DATASET SETUP FOR DISTILBERT TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Step 1: Download (uncomment to download)\n",
    "    # download_fnspid_news()\n",
    "\n",
    "    # Step 2: Load data\n",
    "    df_news = load_fnspid_data()\n",
    "\n",
    "    # Step 3: Filter for recent years\n",
    "    df_recent = filter_recent_data(df_news, 2022, 2024)\n",
    "\n",
    "    # Step 4: Prepare for training\n",
    "    df_prepared = prepare_training_data(df_recent)\n",
    "\n",
    "    # Step 5: Create balanced sample\n",
    "    df_balanced = create_balanced_sample(df_prepared, samples_per_class=3000)\n",
    "\n",
    "    # Save training data\n",
    "    df_balanced.to_csv('fnspid_training_data_2022_2024.csv', index=False)\n",
    "    print(f\"\\n✅ Training data saved to: fnspid_training_data_2022_2024.csv\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. Review the training data quality\")\n",
    "    print(\"2. Use this balanced dataset to train DistilBERT\")\n",
    "    print(\"3. Apply trained model to your Cleantech dataset\")\n",
    "    print(\"4. This should solve your domain mismatch problem!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983bb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FNSPID DATASET SETUP FOR DISTILBERT TRAINING\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_fnspid_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Step 1: Download (uncomment to download)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# download_fnspid_news()\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Step 2: Load data\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m df_news \u001b[38;5;241m=\u001b[39m \u001b[43mload_fnspid_data\u001b[49m()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Step 3: Filter for recent years\u001b[39;00m\n\u001b[1;32m     78\u001b[0m df_recent \u001b[38;5;241m=\u001b[39m filter_recent_data(df_news, \u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m2024\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_fnspid_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Prepare data for DistilBERT training\n",
    "def prepare_training_data(df):\n",
    "    \"\"\"Prepare FNSPID data for DistilBERT fine-tuning\"\"\"\n",
    "\n",
    "    # Create text column combining title and content\n",
    "    if 'title' in df.columns and 'text' in df.columns:\n",
    "        df['combined_text'] = df['title'].fillna('') + \". \" + df['text'].fillna('')\n",
    "    elif 'news_text' in df.columns:\n",
    "        df['combined_text'] = df['news_text'].fillna('')\n",
    "    else:\n",
    "        # Adapt based on actual column names\n",
    "        text_cols = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower()]\n",
    "        if text_cols:\n",
    "            df['combined_text'] = df[text_cols[0]].fillna('')\n",
    "\n",
    "    # Clean sentiment labels (adapt based on actual format)\n",
    "    # FNSPID uses various sentiment scoring methods - normalize to positive/neutral/negative\n",
    "    if 'sentiment_score' in df.columns:\n",
    "        # Convert sentiment scores to labels\n",
    "        df['sentiment_label'] = df['sentiment_score'].apply(lambda x:\n",
    "            'positive' if x > 0.1 else ('negative' if x < -0.1 else 'neutral'))\n",
    "    elif 'sentiment' in df.columns:\n",
    "        df['sentiment_label'] = df['sentiment']\n",
    "\n",
    "    # Filter out articles that are too short or too long\n",
    "    df['text_length'] = df['combined_text'].str.len()\n",
    "    df_clean = df[\n",
    "        (df['text_length'] >= 100) &  # Minimum 100 characters\n",
    "        (df['text_length'] <= 2000) & # Maximum 2000 characters\n",
    "        (df['combined_text'].notna()) &\n",
    "        (df['sentiment_label'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "    print(f\"Text length stats:\")\n",
    "    print(df_clean['text_length'].describe())\n",
    "    print(f\"\\nSentiment distribution:\")\n",
    "    print(df_clean['sentiment_label'].value_counts())\n",
    "\n",
    "    return df_clean[['combined_text', 'sentiment_label', 'date']]\n",
    "\n",
    "# Step 5: Sample balanced training data\n",
    "def create_balanced_sample(df, samples_per_class=5000):\n",
    "    \"\"\"Create a balanced sample for training\"\"\"\n",
    "\n",
    "    balanced_dfs = []\n",
    "\n",
    "    for sentiment in df['sentiment_label'].unique():\n",
    "        sentiment_df = df[df['sentiment_label'] == sentiment]\n",
    "        if len(sentiment_df) >= samples_per_class:\n",
    "            sampled = sentiment_df.sample(n=samples_per_class, random_state=42)\n",
    "        else:\n",
    "            sampled = sentiment_df  # Use all available if less than target\n",
    "        balanced_dfs.append(sampled)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "    print(f\"Balanced training set shape: {balanced_df.shape}\")\n",
    "    print(f\"Sentiment distribution:\")\n",
    "    print(balanced_df['sentiment_label'].value_counts())\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*50)\n",
    "    print(\"FNSPID DATASET SETUP FOR DISTILBERT TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Step 1: Download (uncomment to download)\n",
    "    # download_fnspid_news()\n",
    "\n",
    "    # Step 2: Load data\n",
    "    df_news = load_fnspid_data()\n",
    "\n",
    "    # Step 3: Filter for recent years\n",
    "    df_recent = filter_recent_data(df_news, 2022, 2024)\n",
    "\n",
    "    # Step 4: Prepare for training\n",
    "    df_prepared = prepare_training_data(df_recent)\n",
    "\n",
    "    # Step 5: Create balanced sample\n",
    "    df_balanced = create_balanced_sample(df_prepared, samples_per_class=3000)\n",
    "\n",
    "    # Save training data\n",
    "    df_balanced.to_csv('fnspid_training_data_2022_2024.csv', index=False)\n",
    "    print(f\"\\n✅ Training data saved to: fnspid_training_data_2022_2024.csv\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. Review the training data quality\")\n",
    "    print(\"2. Use this balanced dataset to train DistilBERT\")\n",
    "    print(\"3. Apply trained model to your Cleantech dataset\")\n",
    "    print(\"4. This should solve your domain mismatch problem!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bef1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SolarSoundBytes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
